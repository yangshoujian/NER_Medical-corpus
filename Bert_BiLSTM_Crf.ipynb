{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "ffcjdnPXXO-y",
    "outputId": "6fa2ed34-1344-4bce-be48-f394ddcee270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "Udn2MvHCy7CG",
    "outputId": "f7dec40f-1ba8-4aac-8d21-9b4ac6ab09e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling tensorflow-2.2.0:\n",
      "  Would remove:\n",
      "    /usr/local/bin/estimator_ckpt_converter\n",
      "    /usr/local/bin/saved_model_cli\n",
      "    /usr/local/bin/tensorboard\n",
      "    /usr/local/bin/tf_upgrade_v2\n",
      "    /usr/local/bin/tflite_convert\n",
      "    /usr/local/bin/toco\n",
      "    /usr/local/bin/toco_from_protos\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.2.0.dist-info/*\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
      "Proceed (y/n)? y\n",
      "\n",
      "  Successfully uninstalled tensorflow-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "colab_type": "code",
    "id": "-fAVnSEXyrco",
    "outputId": "7a5c9604-2000-4926-91e9-059ec6fb9115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
      "\u001b[K     |████████████████████████████████| 109.2MB 87kB/s \n",
      "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 37.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.5)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "\u001b[K     |████████████████████████████████| 491kB 44.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.34.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.29.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (47.1.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.1.0)\n",
      "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
      "  Found existing installation: tensorboard 2.2.2\n",
      "    Uninstalling tensorboard-2.2.2:\n",
      "      Successfully uninstalled tensorboard-2.2.2\n",
      "  Found existing installation: tensorflow-estimator 2.2.0\n",
      "    Uninstalling tensorflow-estimator-2.2.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
      "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "colab_type": "code",
    "id": "TeyVvLpyyRSs",
    "outputId": "02e39f2c-fa0a-4e82-c790-6d82e2628429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kashgari==1.1.4 in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
      "Requirement already satisfied: keras-bert>=0.50.0 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (0.84.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (0.22.2.post1)\n",
      "Requirement already satisfied: bert4keras==0.6.5 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (0.6.5)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (1.0.4)\n",
      "Requirement already satisfied: keras-gpt-2>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (0.14.0)\n",
      "Requirement already satisfied: numpy==1.16.4 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (1.16.4)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (2.10.0)\n",
      "Requirement already satisfied: seqeval==0.0.10 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (0.0.10)\n",
      "Requirement already satisfied: gensim>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (3.6.0)\n",
      "Requirement already satisfied: keras-transformer>=0.37.0 in /usr/local/lib/python3.6/dist-packages (from keras-bert>=0.50.0->kashgari==1.1.4) (0.37.0)\n",
      "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert>=0.50.0->kashgari==1.1.4) (2.3.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.1->kashgari==1.1.4) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.1->kashgari==1.1.4) (0.15.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->kashgari==1.1.4) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->kashgari==1.1.4) (2.8.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from keras-gpt-2>=0.8.0->kashgari==1.1.4) (2019.12.20)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->kashgari==1.1.4) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.5.0->kashgari==1.1.4) (2.0.0)\n",
      "Requirement already satisfied: keras-layer-normalization>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert>=0.50.0->kashgari==1.1.4) (0.14.0)\n",
      "Requirement already satisfied: keras-multi-head>=0.27.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert>=0.50.0->kashgari==1.1.4) (0.27.0)\n",
      "Requirement already satisfied: keras-pos-embd>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert>=0.50.0->kashgari==1.1.4) (0.11.0)\n",
      "Requirement already satisfied: keras-embed-sim>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert>=0.50.0->kashgari==1.1.4) (0.7.0)\n",
      "Requirement already satisfied: keras-position-wise-feed-forward>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.37.0->keras-bert>=0.50.0->kashgari==1.1.4) (0.6.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert>=0.50.0->kashgari==1.1.4) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert>=0.50.0->kashgari==1.1.4) (3.13)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert>=0.50.0->kashgari==1.1.4) (1.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (2.23.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.13.23)\n",
      "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (2.49.0)\n",
      "Requirement already satisfied: keras-self-attention==0.46.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.27.0->keras-transformer>=0.37.0->keras-bert>=0.50.0->kashgari==1.1.4) (0.46.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (3.0.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.17.0,>=1.16.23 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.16.23)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (0.10.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.23->boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install kashgari==1.1.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rc9_KUZqaB1Q"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "path=\"/content/drive/My Drive/Colab Notebooks/NLP/Project/\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "gV0-2HPAQgXI",
    "outputId": "3e71f313-366b-4bd4-f00f-7ca02675dde7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'>\n",
      "4164 4164 311 311 787 787\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import kashgari\n",
    "\n",
    "kashgari.config.use_cudnn_cell = True # 使用GPU\n",
    "train = pd.read_json(\"train_clean.txt\")\n",
    "train_x, train_y = train[\"text_clean\"].tolist(), train[\"anno_text3\"].tolist()\n",
    "dev = pd.read_json(\"dev_clean.txt\")\n",
    "dev_x, dev_y = dev[\"text_clean\"].tolist(), dev[\"anno_text3\"].tolist()\n",
    "test = pd.read_json(\"test_clean.txt\")\n",
    "test_x, test_y = test[\"text_clean\"].tolist(), test[\"anno_text3\"].tolist()\n",
    "\n",
    "train.head()\n",
    "print(type(train_x), type(train_y))\n",
    "print(len(train_x),len(train_y),len(dev_x),len(dev_y), len(test_x), len(test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "f72P353W0d-T",
    "outputId": "c9fba2f8-2503-43d0-ede8-f28b1be345a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:22: UserWarning: bert4keras.bert has been renamed as bert4keras.models.\n",
      "  warnings.warn('bert4keras.bert has been renamed as bert4keras.models.')\n",
      "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:23: UserWarning: please use bert4keras.models.\n",
      "  warnings.warn('please use bert4keras.models.')\n",
      "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:44: UserWarning: bert4keras.tokenizer has been renamed as bert4keras.tokenizers.\n",
      "  warnings.warn('bert4keras.tokenizer has been renamed as bert4keras.tokenizers.')\n",
      "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:45: UserWarning: please use bert4keras.tokenizers.\n",
      "  warnings.warn('please use bert4keras.tokenizers.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f6603427518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f6603427518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f6603313f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f6603313f98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6603205a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6603205a20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f66032774e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f66032774e0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6603089470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6603089470>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f66030ec240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f66030ec240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f66030ec4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f66030ec4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6602e84a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6602e84a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f6602e32208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f6602e32208>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6602c0d7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6602c0d7b8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6602dab5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6602dab5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6602cff4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6602cff4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6603184ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6603184ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f660293d630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f660293d630>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f66027de3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f66027de3c8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f66028d3390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f66028d3390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f660288bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f660288bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f66025d1c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f66025d1c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f660250c550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f660250c550>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f660234fe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f660234fe80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f66024b0e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f66024b0e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f66023aca20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f66023aca20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f660213eac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f660213eac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f660207ed30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f660207ed30>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6601ec4e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6601ec4e80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601f23198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601f23198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6601f234e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6601f234e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601f45cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601f45cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f6601c12400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f6601c12400>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6601a37780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6601a37780>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601bd9780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601bd9780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6601b1c780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6601b1c780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601840dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601840dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f66017d3f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f66017d3f98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6601626d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6601626d68>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601603208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601603208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6601603390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6601603390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601354e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601354e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f6601354c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f6601354c50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6601199da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6601199da0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601368860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6601368860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f66011f6320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f66011f6320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6600ec89b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6600ec89b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f6600ec8278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f6600ec8278>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6600d03438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6600d03438>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6600f34c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6600f34c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6600e2c978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6600e2c978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6600ab8da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6600ab8da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f6600ad4ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f6600ad4ac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f660087cda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f660087cda0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f66009dd400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f66009dd400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f66009ac518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f66009ac518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f660062e240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f660062e240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f660062e358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f660062e358>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6600472da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f6600472da0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6600598940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f6600598940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6600521898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6600521898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f660018e048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f660018e048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f660018e4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f660018e4e0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f65fffe3da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f65fffe3da0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f660010be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f660010be10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6600095e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f6600095e80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f65ffd135c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f65ffd135c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:seq_len: 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method NonMaskingLayer.call of <kashgari.layers.non_masking_layer.NonMaskingLayer object at 0x7f668240c438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method NonMaskingLayer.call of <kashgari.layers.non_masking_layer.NonMaskingLayer object at 0x7f668240c438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "from kashgari.embeddings import BERTEmbedding\n",
    "\n",
    "bert_embed = BERTEmbedding('chinese_L-12_H-768_A-12', task=kashgari.LABELING, sequence_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DGmJ5_jn1OiH",
    "outputId": "639e0a51-d247-46b4-e2ab-1efe736276b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:22: UserWarning: bert4keras.bert has been renamed as bert4keras.models.\n",
      "  warnings.warn('bert4keras.bert has been renamed as bert4keras.models.')\n",
      "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:23: UserWarning: please use bert4keras.models.\n",
      "  warnings.warn('please use bert4keras.models.')\n",
      "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:44: UserWarning: bert4keras.tokenizer has been renamed as bert4keras.tokenizers.\n",
      "  warnings.warn('bert4keras.tokenizer has been renamed as bert4keras.tokenizers.')\n",
      "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:45: UserWarning: please use bert4keras.tokenizers.\n",
      "  warnings.warn('please use bert4keras.tokenizers.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method CRF.call of <kashgari.layers.crf.CRF object at 0x7f1f219dc048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CRF.call of <kashgari.layers.crf.CRF object at 0x7f1f219dc048>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 500, 768), ( 16226304    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 500, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 500, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 500, 768)     384000      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 500, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 500, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 500, 768)     2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 500, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 500, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 500, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 500, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 500, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 500, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 500, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 500, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 500, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 500, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 500, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 500, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 500, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 500, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 500, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 500, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 500, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 500, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 500, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 500, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 500, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 500, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 500, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 500, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 500, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 500, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 500, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 500, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 500, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 500, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 500, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 500, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 500, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 500, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 500, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 500, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 500, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 500, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 500, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 500, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 500, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 500, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 500, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 500, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 500, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 500, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 500, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 500, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 500, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 500, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 500, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 500, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 500, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 500, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 500, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 500, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 500, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 500, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 500, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 500, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 500, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 500, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 500, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 500, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 500, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 500, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 500, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 500, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 500, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 500, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 500, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 500, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 500, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 500, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 500, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 500, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 500, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 500, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 500, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output (Concatenate)    (None, 500, 3072)    0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "non_masking_layer_1 (NonMasking (None, 500, 3072)    0           Encoder-Output[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_blstm (Bidirectional)     (None, 500, 256)     3278848     non_masking_layer_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_dense (Dense)             (None, 500, 64)      16448       layer_blstm[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_crf_dense (Dense)         (None, 500, 14)      910         layer_dense[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_crf (CRF)                 (None, 500, 14)      196         layer_crf_dense[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 104,964,242\n",
      "Trainable params: 3,296,402\n",
      "Non-trainable params: 101,667,840\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "66/66 [==============================] - 536s 8s/step - loss: 104.4060 - accuracy: 0.9404 - val_loss: 745.7944 - val_accuracy: 0.9582\n",
      "Epoch 2/20\n",
      "66/66 [==============================] - 528s 8s/step - loss: 57.9216 - accuracy: 0.9608 - val_loss: 736.1374 - val_accuracy: 0.9623\n",
      "Epoch 3/20\n",
      "66/66 [==============================] - 528s 8s/step - loss: 49.5410 - accuracy: 0.9629 - val_loss: 728.9810 - val_accuracy: 0.9621\n",
      "Epoch 4/20\n",
      "66/66 [==============================] - 527s 8s/step - loss: 46.1149 - accuracy: 0.9640 - val_loss: 719.1469 - val_accuracy: 0.9646\n",
      "Epoch 5/20\n",
      "66/66 [==============================] - 528s 8s/step - loss: 42.1335 - accuracy: 0.9649 - val_loss: 715.3901 - val_accuracy: 0.9619\n",
      "Epoch 6/20\n",
      "66/66 [==============================] - 527s 8s/step - loss: 39.1540 - accuracy: 0.9659 - val_loss: 707.9303 - val_accuracy: 0.9633\n",
      "Epoch 7/20\n",
      "66/66 [==============================] - 527s 8s/step - loss: 36.4475 - accuracy: 0.9670 - val_loss: 703.4475 - val_accuracy: 0.9615\n",
      "Epoch 8/20\n",
      "66/66 [==============================] - 527s 8s/step - loss: 34.0802 - accuracy: 0.9679 - val_loss: 698.2982 - val_accuracy: 0.9597\n",
      "Epoch 9/20\n",
      "66/66 [==============================] - 527s 8s/step - loss: 32.1746 - accuracy: 0.9692 - val_loss: 690.0012 - val_accuracy: 0.9628\n",
      "Epoch 10/20\n",
      "66/66 [==============================] - 527s 8s/step - loss: 30.5795 - accuracy: 0.9694 - val_loss: 686.2379 - val_accuracy: 0.9605\n",
      "Epoch 11/20\n",
      "66/66 [==============================] - 526s 8s/step - loss: 28.7788 - accuracy: 0.9709 - val_loss: 679.8728 - val_accuracy: 0.9626\n",
      "Epoch 12/20\n",
      "66/66 [==============================] - 527s 8s/step - loss: 27.1264 - accuracy: 0.9720 - val_loss: 673.9749 - val_accuracy: 0.9650\n",
      "Epoch 13/20\n",
      "66/66 [==============================] - 527s 8s/step - loss: 25.5981 - accuracy: 0.9727 - val_loss: 671.2706 - val_accuracy: 0.9602\n",
      "Epoch 14/20\n",
      "66/66 [==============================] - 527s 8s/step - loss: 24.6960 - accuracy: 0.9739 - val_loss: 665.3423 - val_accuracy: 0.9628\n",
      "Epoch 15/20\n",
      "66/66 [==============================] - 527s 8s/step - loss: 25.2665 - accuracy: 0.9721 - val_loss: 663.9563 - val_accuracy: 0.9587\n",
      "Epoch 16/20\n",
      "66/66 [==============================] - 524s 8s/step - loss: 22.0575 - accuracy: 0.9753 - val_loss: 658.2594 - val_accuracy: 0.9622\n",
      "Epoch 17/20\n",
      "66/66 [==============================] - 523s 8s/step - loss: 20.8509 - accuracy: 0.9763 - val_loss: 655.3354 - val_accuracy: 0.9599\n",
      "Epoch 18/20\n",
      "66/66 [==============================] - 523s 8s/step - loss: 19.8882 - accuracy: 0.9771 - val_loss: 650.8376 - val_accuracy: 0.9641\n",
      "Epoch 19/20\n",
      "66/66 [==============================] - 523s 8s/step - loss: 18.7401 - accuracy: 0.9784 - val_loss: 649.2792 - val_accuracy: 0.9571\n",
      "Epoch 20/20\n",
      "66/66 [==============================] - 524s 8s/step - loss: 18.0779 - accuracy: 0.9790 - val_loss: 647.0837 - val_accuracy: 0.9551\n"
     ]
    }
   ],
   "source": [
    "from kashgari.tasks.labeling import BiLSTM_CRF_Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = BiLSTM_CRF_Model(bert_embed)\n",
    "\n",
    "# Saving the best model only\n",
    "# filepath=\"ner-bert-bilstm-crf-model.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(train_x, train_y, x_validate=dev_x, y_validate=dev_y, epochs=20, batch_size=64)\n",
    "model.save(\"ner-bert-bilstm-crf-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "F3eYJuqvYJY9",
    "outputId": "c1dd1235-30b3-4d65-dfa2-5f1fbc7aa3a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 0\n",
      "787 787\n",
      "['B-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# pred和test部分长度不同，进行填充\n",
    "test_pred = model.predict(test_x) \n",
    "count1, count2 = 0, 0\n",
    "pred_final = []\n",
    "for test_tag, pred_tag in zip(test_y,test_pred): #将对象中对应的元素打包成一个个元组,然后返回由这些元组组成的列表。\n",
    "  if len(test_tag) > len(pred_tag): # 填充predtag\n",
    "    count1 += 1\n",
    "    pred_final.append(pred_tag+['O']*(len(test_tag)-len(pred_tag)))\n",
    "  elif len(test_tag) < len(pred_tag): # 不存在这种情况\n",
    "    count2 += 1\n",
    "  else:\n",
    "    pred_final.append(pred_tag)\n",
    "print(count1, count2)\n",
    "print(len(test_pred), len(test_y))\n",
    "print(pred_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GBIUmqNLHpJr",
    "outputId": "66cda7b5-3672-480c-d459-1db87ae04b3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_blstm': {'units': 128, 'return_sequences': True}, 'layer_dense': {'units': 64, 'activation': 'tanh'}}\n"
     ]
    }
   ],
   "source": [
    "import kashgari\n",
    "from kashgari.tasks.labeling import BiLSTM_CRF_Model\n",
    "\n",
    "hyper = BiLSTM_CRF_Model.get_default_hyper_parameters() # 超参数\n",
    "print(hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLcIhy5LnSi-"
   },
   "outputs": [],
   "source": [
    "# 直接加载模型\n",
    "import kashgari\n",
    "from kashgari.tasks.labeling import BiLSTM_CRF_Model\n",
    "\n",
    "hyper = BiLSTM_CRF_Model.get_default_hyper_parameters() # 超参数\n",
    "print(hyper)\n",
    "\n",
    "# loaded_model = BiLSTM_CRF_Model(bert_embed)\n",
    "# loaded_model.load_weights('ner-bert-bilstm-crf-model.hdf5')\n",
    "loaded_model = kashgari.utils.load_model('ner-bert-bilstm-crf-model.h5')\n",
    "test_pred = loaded_model.predict(test_x)\n",
    "count1, count2 = 0, 0\n",
    "pred_final = []\n",
    "for test_tag, pred_tag in zip(test_y,test_pred):\n",
    "  if len(test_tag) > len(pred_tag): # 填充predtag\n",
    "    count1 += 1\n",
    "    pred_final.append(pred_tag+['O']*(len(test_tag)-len(pred_tag)))\n",
    "  elif len(test_tag) < len(pred_tag): # 不存在这种情况\n",
    "    count2 += 1\n",
    "  else:\n",
    "    pred_final.append(pred_tag)\n",
    "print(count1, count2)\n",
    "print(len(test_pred), len(test_y))\n",
    "print(pred_final[0])\n",
    "print(test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "m-HwQejdFLXd",
    "outputId": "98be7410-11ed-4689-c55d-e9db9a4ee8ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred0 ['B-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "test0 ['B-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "pred1 ['B-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'I-body', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'O', 'B-body', 'I-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'B-subject', 'I-subject', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'B-decorate', 'I-decorate', 'I-decorate', 'B-subject', 'I-subject', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-item', 'I-item', 'I-item', 'I-item', 'I-item', 'I-item', 'I-item', 'B-subject', 'I-subject', 'O']\n",
      "test1 ['B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'O', 'B-body', 'O', 'B-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'B-subject', 'I-subject', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'B-decorate', 'I-decorate', 'I-decorate', 'I-decorate', 'B-body', 'I-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O']\n",
      "pred2 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "test2 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "pred3 ['B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'B-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-disease', 'I-disease', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'B-disease', 'I-disease', 'I-disease', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'B-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'B-body', 'I-body', 'B-subject', 'I-subject', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-item', 'I-item', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "test3 ['B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'B-body', 'O', 'B-body', 'O', 'B-body', 'I-body', 'B-subject', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-decorate', 'I-decorate', 'I-decorate', 'B-subject', 'I-subject', 'I-subject', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'B-subject', 'I-subject', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'I-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "pred4 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'O', 'O']\n",
      "test4 ['O', 'O', 'B-item', 'I-item', 'I-item', 'I-item', 'I-item', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-subject', 'I-subject', 'O']\n"
     ]
    }
   ],
   "source": [
    "# 分析test前5个text\n",
    "for i in range(5):\n",
    "  print(\"pred\"+str(i),pred_final[i])\n",
    "  print(\"test\"+str(i),test_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "nC5VKPpeTEL2",
    "outputId": "71637b71-c387-484c-be60-2ff0cf5defb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'O', 'B-body', 'O', 'B-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'B-subject', 'I-subject', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'B-decorate', 'I-decorate', 'I-decorate', 'I-decorate', 'B-body', 'I-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'B-body', 'O', 'B-body', 'O', 'B-body', 'I-body', 'B-subject', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-decorate', 'I-decorate', 'I-decorate', 'B-subject', 'I-subject', 'I-subject', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'B-subject', 'I-subject', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'I-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-item', 'I-item', 'I-item', 'I-item', 'I-item', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-subject', 'I-subject', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yQ3oKXhzSNJW",
    "outputId": "de67ab95-cec1-48b0-d7cb-c4852aeac8af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************第0个数据******************\n",
      "text:  GSD依其所缺陷的酶12型，多数属分解代谢上的缺陷，使糖原异常堆积。除GSDⅨb型为X连锁隐性遗传外，其余都是常染色体隐性遗传性疾病。\n",
      "true label:  ['B-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "pred label:  ['B-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[('G', 'B-disease', 'B-disease'), ('S', 'I-disease', 'I-disease'), ('D', 'I-disease', 'I-disease'), ('依', 'O', 'O'), ('其', 'O', 'O'), ('所', 'O', 'O'), ('缺', 'O', 'O'), ('陷', 'O', 'O'), ('的', 'O', 'O'), ('酶', 'B-body', 'O'), ('1', 'I-body', 'O'), ('2', 'I-body', 'O'), ('型', 'I-body', 'O'), ('，', 'O', 'O'), ('多', 'O', 'O'), ('数', 'O', 'O'), ('属', 'O', 'O'), ('分', 'O', 'O'), ('解', 'O', 'O'), ('代', 'O', 'O'), ('谢', 'O', 'O'), ('上', 'O', 'O'), ('的', 'O', 'O'), ('缺', 'B-subject', 'O'), ('陷', 'I-subject', 'O'), ('，', 'O', 'O'), ('使', 'O', 'O'), ('糖', 'B-body', 'B-body'), ('原', 'I-body', 'I-body'), ('异', 'B-subject', 'B-subject'), ('常', 'I-subject', 'I-subject'), ('堆', 'I-subject', 'I-subject'), ('积', 'I-subject', 'I-subject'), ('。', 'O', 'O'), ('除', 'O', 'O'), ('G', 'O', 'O'), ('S', 'O', 'O'), ('D', 'O', 'O'), ('Ⅸ', 'O', 'O'), ('b', 'O', 'O'), ('型', 'O', 'O'), ('为', 'O', 'O'), ('X', 'O', 'O'), ('连', 'O', 'O'), ('锁', 'O', 'O'), ('隐', 'O', 'O'), ('性', 'O', 'O'), ('遗', 'O', 'O'), ('传', 'O', 'O'), ('外', 'O', 'O'), ('，', 'O', 'O'), ('其', 'O', 'O'), ('余', 'O', 'O'), ('都', 'O', 'O'), ('是', 'O', 'O'), ('常', 'O', 'O'), ('染', 'O', 'O'), ('色', 'O', 'O'), ('体', 'O', 'O'), ('隐', 'O', 'O'), ('性', 'O', 'O'), ('遗', 'O', 'O'), ('传', 'O', 'O'), ('性', 'O', 'O'), ('疾', 'O', 'O'), ('病', 'O', 'O'), ('。', 'O', 'O')]\n",
      "0 G B-disease B-disease\n",
      "1 S I-disease I-disease\n",
      "2 D I-disease I-disease\n",
      "27 糖 B-body B-body\n",
      "28 原 I-body I-body\n",
      "29 异 B-subject B-subject\n",
      "30 常 I-subject I-subject\n",
      "31 堆 I-subject I-subject\n",
      "32 积 I-subject I-subject\n",
      "F1-score: 75.00%\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "     body     1.0000    0.5000    0.6667         2\n",
      "  subject     1.0000    0.5000    0.6667         2\n",
      "  disease     1.0000    1.0000    1.0000         1\n",
      "\n",
      "micro avg     1.0000    0.6000    0.7500         5\n",
      "macro avg     1.0000    0.6000    0.7333         5\n",
      "\n",
      "******************第1个数据******************\n",
      "text:  GSDⅧ型酶缺陷不明，肝脏磷酸酶为无活性形式。受累组织发现大脑糖原增加，电镜显示在大脑神经突触和轴突有糖原堆积形成的α颗粒。肝、脑及骨骼肌正常。临床症状和体征：肝肿大、躯体运动失调，眼球震颤，逐渐出现神经系统退行性变化、痉挛，直至死亡。在疾病急性发作期尿儿茶酚胺排量增加。\n",
      "true label:  ['B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'O', 'B-body', 'O', 'B-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'B-subject', 'I-subject', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'B-decorate', 'I-decorate', 'I-decorate', 'I-decorate', 'B-body', 'I-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O']\n",
      "pred label:  ['B-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'I-body', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'O', 'B-body', 'I-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'B-subject', 'I-subject', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'B-decorate', 'I-decorate', 'I-decorate', 'B-subject', 'I-subject', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-item', 'I-item', 'I-item', 'I-item', 'I-item', 'I-item', 'I-item', 'B-subject', 'I-subject', 'O']\n",
      "[('G', 'B-disease', 'B-body'), ('S', 'I-disease', 'I-body'), ('D', 'I-disease', 'I-body'), ('Ⅷ', 'I-disease', 'I-body'), ('型', 'I-disease', 'I-body'), ('酶', 'I-disease', 'I-body'), ('缺', 'I-disease', 'B-subject'), ('陷', 'I-disease', 'I-subject'), ('不', 'O', 'I-subject'), ('明', 'O', 'I-subject'), ('，', 'O', 'O'), ('肝', 'O', 'B-body'), ('脏', 'O', 'I-body'), ('磷', 'O', 'I-body'), ('酸', 'O', 'I-body'), ('酶', 'O', 'I-body'), ('为', 'O', 'O'), ('无', 'O', 'O'), ('活', 'O', 'O'), ('性', 'O', 'O'), ('形', 'O', 'O'), ('式', 'O', 'O'), ('。', 'O', 'O'), ('受', 'O', 'O'), ('累', 'O', 'O'), ('组', 'O', 'O'), ('织', 'O', 'O'), ('发', 'O', 'O'), ('现', 'O', 'O'), ('大', 'O', 'B-body'), ('脑', 'O', 'I-body'), ('糖', 'O', 'I-body'), ('原', 'O', 'I-body'), ('增', 'O', 'B-subject'), ('加', 'O', 'I-subject'), ('，', 'O', 'O'), ('电', 'O', 'O'), ('镜', 'O', 'O'), ('显', 'O', 'O'), ('示', 'O', 'O'), ('在', 'O', 'O'), ('大', 'B-body', 'B-body'), ('脑', 'I-body', 'I-body'), ('神', 'I-body', 'I-body'), ('经', 'I-body', 'I-body'), ('突', 'I-body', 'I-body'), ('触', 'I-body', 'I-body'), ('和', 'I-body', 'O'), ('轴', 'I-body', 'B-body'), ('突', 'I-body', 'I-body'), ('有', 'B-subject', 'O'), ('糖', 'I-subject', 'B-body'), ('原', 'I-subject', 'I-body'), ('堆', 'I-subject', 'B-subject'), ('积', 'I-subject', 'I-subject'), ('形', 'I-subject', 'O'), ('成', 'I-subject', 'O'), ('的', 'I-subject', 'O'), ('α', 'I-subject', 'B-subject'), ('颗', 'I-subject', 'I-subject'), ('粒', 'I-subject', 'I-subject'), ('。', 'O', 'O'), ('肝', 'B-body', 'O'), ('、', 'O', 'O'), ('脑', 'B-body', 'O'), ('及', 'O', 'O'), ('骨', 'B-body', 'B-body'), ('骼', 'I-body', 'I-body'), ('肌', 'I-body', 'I-body'), ('正', 'B-subject', 'O'), ('常', 'I-subject', 'O'), ('。', 'O', 'O'), ('临', 'O', 'O'), ('床', 'O', 'O'), ('症', 'O', 'O'), ('状', 'O', 'O'), ('和', 'O', 'O'), ('体', 'O', 'O'), ('征', 'O', 'O'), ('：', 'O', 'O'), ('肝', 'B-body', 'B-body'), ('肿', 'B-subject', 'B-subject'), ('大', 'I-subject', 'I-subject'), ('、', 'O', 'O'), ('躯', 'B-body', 'B-body'), ('体', 'I-body', 'I-body'), ('运', 'B-subject', 'B-subject'), ('动', 'I-subject', 'I-subject'), ('失', 'I-subject', 'I-subject'), ('调', 'I-subject', 'I-subject'), ('，', 'O', 'O'), ('眼', 'B-body', 'B-body'), ('球', 'I-body', 'I-body'), ('震', 'B-subject', 'B-subject'), ('颤', 'I-subject', 'I-subject'), ('，', 'O', 'O'), ('逐', 'B-decorate', 'O'), ('渐', 'I-decorate', 'O'), ('出', 'I-decorate', 'O'), ('现', 'I-decorate', 'O'), ('神', 'B-body', 'B-body'), ('经', 'I-body', 'I-body'), ('系', 'I-body', 'I-body'), ('统', 'I-body', 'I-body'), ('退', 'B-subject', 'B-decorate'), ('行', 'I-subject', 'I-decorate'), ('性', 'I-subject', 'I-decorate'), ('变', 'I-subject', 'B-subject'), ('化', 'I-subject', 'I-subject'), ('、', 'I-subject', 'O'), ('痉', 'I-subject', 'B-subject'), ('挛', 'I-subject', 'I-subject'), ('，', 'O', 'O'), ('直', 'O', 'O'), ('至', 'O', 'O'), ('死', 'B-subject', 'B-subject'), ('亡', 'I-subject', 'I-subject'), ('。', 'O', 'O'), ('在', 'O', 'O'), ('疾', 'O', 'O'), ('病', 'O', 'O'), ('急', 'O', 'O'), ('性', 'O', 'O'), ('发', 'O', 'O'), ('作', 'O', 'O'), ('期', 'O', 'O'), ('尿', 'O', 'B-item'), ('儿', 'O', 'I-item'), ('茶', 'O', 'I-item'), ('酚', 'O', 'I-item'), ('胺', 'O', 'I-item'), ('排', 'B-subject', 'I-item'), ('量', 'I-subject', 'I-item'), ('增', 'I-subject', 'B-subject'), ('加', 'I-subject', 'I-subject'), ('。', 'O', 'O')]\n",
      "41 大 B-body B-body\n",
      "42 脑 I-body I-body\n",
      "43 神 I-body I-body\n",
      "44 经 I-body I-body\n",
      "45 突 I-body I-body\n",
      "46 触 I-body I-body\n",
      "49 突 I-body I-body\n",
      "54 积 I-subject I-subject\n",
      "59 颗 I-subject I-subject\n",
      "60 粒 I-subject I-subject\n",
      "66 骨 B-body B-body\n",
      "67 骼 I-body I-body\n",
      "68 肌 I-body I-body\n",
      "80 肝 B-body B-body\n",
      "81 肿 B-subject B-subject\n",
      "82 大 I-subject I-subject\n",
      "84 躯 B-body B-body\n",
      "85 体 I-body I-body\n",
      "86 运 B-subject B-subject\n",
      "87 动 I-subject I-subject\n",
      "88 失 I-subject I-subject\n",
      "89 调 I-subject I-subject\n",
      "91 眼 B-body B-body\n",
      "92 球 I-body I-body\n",
      "93 震 B-subject B-subject\n",
      "94 颤 I-subject I-subject\n",
      "100 神 B-body B-body\n",
      "101 经 I-body I-body\n",
      "102 系 I-body I-body\n",
      "103 统 I-body I-body\n",
      "108 化 I-subject I-subject\n",
      "111 挛 I-subject I-subject\n",
      "115 死 B-subject B-subject\n",
      "116 亡 I-subject I-subject\n",
      "134 加 I-subject I-subject\n",
      "F1-score: 42.86%\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "     body     0.4545    0.6250    0.5263         8\n",
      "  subject     0.3636    0.5000    0.4211         8\n",
      "  disease     0.0000    0.0000    0.0000         1\n",
      " decorate     0.0000    0.0000    0.0000         1\n",
      "\n",
      "micro avg     0.3750    0.5000    0.4286        18\n",
      "macro avg     0.3636    0.5000    0.4211        18\n",
      "\n",
      "******************第2个数据******************\n",
      "text:  临床严重程度不同，其预后亦不同。一般来说，Ⅰ型糖原累积病较难处理；年龄越小、症状越重，其预后差，常因感染及酸中毒而使病情进展迅速。4岁后，临床症状可减轻。有的患者并发心脏扩大，以后死于心力衰竭。青春期后时有痛风发作。国外研究曾随访调查43例Ⅰ型和Ⅲ型糖原累积病，发现51.8%Ⅰ型患者及25%Ⅲ型患者以后发生肝脏肿瘤，这些患者血清α-甲胎蛋白水平明显增高。因此对于糖原累积病患者每年进行肝脏超声检查及定期测定血清α-甲胎蛋白。\n",
      "true label:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "pred label:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[('临', 'O', 'O'), ('床', 'O', 'O'), ('严', 'O', 'O'), ('重', 'O', 'O'), ('程', 'O', 'O'), ('度', 'O', 'O'), ('不', 'O', 'O'), ('同', 'O', 'O'), ('，', 'O', 'O'), ('其', 'O', 'O'), ('预', 'O', 'O'), ('后', 'O', 'O'), ('亦', 'O', 'O'), ('不', 'O', 'O'), ('同', 'O', 'O'), ('。', 'O', 'O'), ('一', 'O', 'O'), ('般', 'O', 'O'), ('来', 'O', 'O'), ('说', 'O', 'O'), ('，', 'O', 'O'), ('Ⅰ', 'B-disease', 'O'), ('型', 'I-disease', 'O'), ('糖', 'I-disease', 'O'), ('原', 'I-disease', 'O'), ('累', 'I-disease', 'O'), ('积', 'I-disease', 'O'), ('病', 'I-disease', 'O'), ('较', 'O', 'O'), ('难', 'O', 'O'), ('处', 'O', 'O'), ('理', 'O', 'O'), ('；', 'O', 'O'), ('年', 'O', 'O'), ('龄', 'O', 'O'), ('越', 'O', 'O'), ('小', 'O', 'O'), ('、', 'O', 'O'), ('症', 'O', 'O'), ('状', 'O', 'O'), ('越', 'O', 'O'), ('重', 'O', 'O'), ('，', 'O', 'O'), ('其', 'O', 'O'), ('预', 'O', 'O'), ('后', 'O', 'O'), ('差', 'O', 'O'), ('，', 'O', 'O'), ('常', 'O', 'O'), ('因', 'O', 'O'), ('感', 'O', 'O'), ('染', 'O', 'O'), ('及', 'O', 'O'), ('酸', 'O', 'O'), ('中', 'O', 'O'), ('毒', 'O', 'O'), ('而', 'O', 'O'), ('使', 'O', 'O'), ('病', 'O', 'O'), ('情', 'O', 'O'), ('进', 'O', 'O'), ('展', 'O', 'O'), ('迅', 'O', 'O'), ('速', 'O', 'O'), ('。', 'O', 'O'), ('4', 'O', 'O'), ('岁', 'O', 'O'), ('后', 'O', 'O'), ('，', 'O', 'O'), ('临', 'O', 'O'), ('床', 'O', 'O'), ('症', 'O', 'O'), ('状', 'O', 'O'), ('可', 'O', 'O'), ('减', 'O', 'O'), ('轻', 'O', 'O'), ('。', 'O', 'O'), ('有', 'O', 'O'), ('的', 'O', 'O'), ('患', 'O', 'O'), ('者', 'O', 'O'), ('并', 'O', 'O'), ('发', 'O', 'O'), ('心', 'O', 'B-body'), ('脏', 'O', 'I-body'), ('扩', 'O', 'B-subject'), ('大', 'O', 'I-subject'), ('，', 'O', 'O'), ('以', 'O', 'O'), ('后', 'O', 'O'), ('死', 'B-subject', 'O'), ('于', 'I-subject', 'O'), ('心', 'I-subject', 'O'), ('力', 'I-subject', 'O'), ('衰', 'I-subject', 'O'), ('竭', 'I-subject', 'O'), ('。', 'O', 'O'), ('青', 'O', 'O'), ('春', 'O', 'O'), ('期', 'O', 'O'), ('后', 'O', 'O'), ('时', 'O', 'O'), ('有', 'O', 'O'), ('痛', 'O', 'B-subject'), ('风', 'O', 'I-subject'), ('发', 'O', 'O'), ('作', 'O', 'O'), ('。', 'O', 'O'), ('国', 'O', 'O'), ('外', 'O', 'O'), ('研', 'O', 'O'), ('究', 'O', 'O'), ('曾', 'O', 'O'), ('随', 'O', 'O'), ('访', 'O', 'O'), ('调', 'O', 'O'), ('查', 'O', 'O'), ('4', 'O', 'O'), ('3', 'O', 'O'), ('例', 'O', 'O'), ('Ⅰ', 'O', 'O'), ('型', 'O', 'O'), ('和', 'O', 'O'), ('Ⅲ', 'O', 'O'), ('型', 'O', 'O'), ('糖', 'O', 'O'), ('原', 'O', 'O'), ('累', 'O', 'O'), ('积', 'O', 'O'), ('病', 'O', 'O'), ('，', 'O', 'O'), ('发', 'O', 'O'), ('现', 'O', 'O'), ('5', 'O', 'O'), ('1', 'O', 'O'), ('.', 'O', 'O'), ('8', 'O', 'O'), ('%', 'O', 'O'), ('Ⅰ', 'O', 'O'), ('型', 'O', 'O'), ('患', 'O', 'O'), ('者', 'O', 'O'), ('及', 'O', 'O'), ('2', 'O', 'O'), ('5', 'O', 'O'), ('%', 'O', 'O'), ('Ⅲ', 'O', 'O'), ('型', 'O', 'O'), ('患', 'O', 'O'), ('者', 'O', 'O'), ('以', 'O', 'O'), ('后', 'O', 'O'), ('发', 'O', 'O'), ('生', 'O', 'O'), ('肝', 'O', 'O'), ('脏', 'O', 'O'), ('肿', 'O', 'O'), ('瘤', 'O', 'O'), ('，', 'O', 'O'), ('这', 'O', 'O'), ('些', 'O', 'O'), ('患', 'O', 'O'), ('者', 'O', 'O'), ('血', 'O', 'O'), ('清', 'O', 'O'), ('α', 'O', 'O'), ('-', 'O', 'O'), ('甲', 'O', 'O'), ('胎', 'O', 'O'), ('蛋', 'O', 'O'), ('白', 'O', 'O'), ('水', 'O', 'O'), ('平', 'O', 'O'), ('明', 'O', 'O'), ('显', 'O', 'O'), ('增', 'O', 'O'), ('高', 'O', 'O'), ('。', 'O', 'O'), ('因', 'O', 'O'), ('此', 'O', 'O'), ('对', 'O', 'O'), ('于', 'O', 'O'), ('糖', 'O', 'O'), ('原', 'O', 'O'), ('累', 'O', 'O'), ('积', 'O', 'O'), ('病', 'O', 'O'), ('患', 'O', 'O'), ('者', 'O', 'O'), ('每', 'O', 'O'), ('年', 'O', 'O'), ('进', 'O', 'O'), ('行', 'O', 'O'), ('肝', 'O', 'O'), ('脏', 'O', 'O'), ('超', 'O', 'O'), ('声', 'O', 'O'), ('检', 'O', 'O'), ('查', 'O', 'O'), ('及', 'O', 'O'), ('定', 'O', 'O'), ('期', 'O', 'O'), ('测', 'O', 'O'), ('定', 'O', 'O'), ('血', 'O', 'O'), ('清', 'O', 'O'), ('α', 'O', 'O'), ('-', 'O', 'O'), ('甲', 'O', 'O'), ('胎', 'O', 'O'), ('蛋', 'O', 'O'), ('白', 'O', 'O'), ('。', 'O', 'O')]\n",
      "F1-score: 0.00%\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "  disease     0.0000    0.0000    0.0000         1\n",
      "  subject     0.0000    0.0000    0.0000         1\n",
      "\n",
      "micro avg     0.0000    0.0000    0.0000         2\n",
      "macro avg     0.0000    0.0000    0.0000         2\n",
      "\n",
      "******************第3个数据******************\n",
      "text:  GSDⅠa型（VonGierke病）葡萄糖-6-磷酸酶缺陷。受累组织为肝、肾和小肠低血糖症状，呼吸困难，乳酸性酸中毒、酮血症，高脂血症、高尿酸血症，血小板功能障碍有出血倾向。患儿呈娃娃脸，肝脏、肾脏肿大身材矮小智能发育正常。半乳糖和果糖不能转变成葡萄糖并能导致酸中毒。注射肾上腺素或胰高肾上腺素后对血糖浓度无改血糖浓度儿茶酚胺正常。\n",
      "true label:  ['B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'B-body', 'O', 'B-body', 'O', 'B-body', 'I-body', 'B-subject', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-decorate', 'I-decorate', 'I-decorate', 'B-subject', 'I-subject', 'I-subject', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'B-subject', 'I-subject', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'I-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "pred label:  ['B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'B-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-disease', 'I-disease', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'B-disease', 'I-disease', 'I-disease', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'B-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'B-body', 'I-body', 'B-subject', 'I-subject', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-item', 'I-item', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[('G', 'B-disease', 'B-disease'), ('S', 'I-disease', 'I-disease'), ('D', 'I-disease', 'I-disease'), ('Ⅰ', 'I-disease', 'I-disease'), ('a', 'I-disease', 'I-disease'), ('型', 'I-disease', 'I-disease'), ('（', 'I-disease', 'I-disease'), ('V', 'I-disease', 'I-disease'), ('o', 'I-disease', 'I-disease'), ('n', 'I-disease', 'I-disease'), ('G', 'I-disease', 'I-disease'), ('i', 'I-disease', 'I-disease'), ('e', 'I-disease', 'I-disease'), ('r', 'I-disease', 'I-disease'), ('k', 'I-disease', 'I-disease'), ('e', 'I-disease', 'I-disease'), ('病', 'I-disease', 'I-disease'), ('）', 'I-disease', 'O'), ('葡', 'I-disease', 'O'), ('萄', 'I-disease', 'O'), ('糖', 'I-disease', 'O'), ('-', 'I-disease', 'I-disease'), ('6', 'I-disease', 'I-disease'), ('-', 'I-disease', 'I-disease'), ('磷', 'I-disease', 'I-disease'), ('酸', 'I-disease', 'I-disease'), ('酶', 'I-disease', 'I-disease'), ('缺', 'I-disease', 'I-disease'), ('陷', 'I-disease', 'I-disease'), ('。', 'O', 'O'), ('受', 'B-subject', 'O'), ('累', 'I-subject', 'O'), ('组', 'O', 'O'), ('织', 'O', 'O'), ('为', 'O', 'O'), ('肝', 'B-body', 'B-body'), ('、', 'O', 'I-body'), ('肾', 'B-body', 'B-body'), ('和', 'O', 'O'), ('小', 'B-body', 'B-body'), ('肠', 'I-body', 'I-body'), ('低', 'B-subject', 'B-subject'), ('血', 'O', 'I-disease'), ('糖', 'O', 'I-disease'), ('症', 'O', 'O'), ('状', 'O', 'O'), ('，', 'O', 'O'), ('呼', 'B-subject', 'B-subject'), ('吸', 'I-subject', 'I-subject'), ('困', 'I-subject', 'I-subject'), ('难', 'I-subject', 'I-subject'), ('，', 'O', 'O'), ('乳', 'B-decorate', 'B-disease'), ('酸', 'I-decorate', 'I-disease'), ('性', 'I-decorate', 'I-disease'), ('酸', 'B-subject', 'I-disease'), ('中', 'I-subject', 'I-disease'), ('毒', 'I-subject', 'I-disease'), ('、', 'O', 'O'), ('酮', 'B-subject', 'B-disease'), ('血', 'I-subject', 'I-disease'), ('症', 'I-subject', 'I-disease'), ('，', 'O', 'O'), ('高', 'B-subject', 'B-disease'), ('脂', 'I-subject', 'I-disease'), ('血', 'I-subject', 'I-disease'), ('症', 'I-subject', 'I-disease'), ('、', 'O', 'O'), ('高', 'O', 'B-disease'), ('尿', 'O', 'I-disease'), ('酸', 'O', 'I-disease'), ('血', 'O', 'I-disease'), ('症', 'O', 'I-disease'), ('，', 'O', 'O'), ('血', 'B-body', 'B-body'), ('小', 'I-body', 'I-body'), ('板', 'I-body', 'I-body'), ('功', 'B-subject', 'B-subject'), ('能', 'I-subject', 'I-subject'), ('障', 'I-subject', 'I-subject'), ('碍', 'I-subject', 'I-subject'), ('有', 'B-subject', 'O'), ('出', 'I-subject', 'O'), ('血', 'I-subject', 'O'), ('倾', 'I-subject', 'O'), ('向', 'I-subject', 'O'), ('。', 'O', 'O'), ('患', 'O', 'O'), ('儿', 'O', 'O'), ('呈', 'O', 'O'), ('娃', 'B-subject', 'O'), ('娃', 'I-subject', 'O'), ('脸', 'I-subject', 'O'), ('，', 'O', 'O'), ('肝', 'B-body', 'B-body'), ('脏', 'I-body', 'I-body'), ('、', 'O', 'O'), ('肾', 'B-body', 'B-body'), ('脏', 'I-body', 'I-body'), ('肿', 'B-subject', 'B-subject'), ('大', 'I-subject', 'I-subject'), ('身', 'O', 'B-body'), ('材', 'O', 'I-body'), ('矮', 'B-subject', 'B-subject'), ('小', 'I-subject', 'I-subject'), ('智', 'B-subject', 'B-subject'), ('能', 'I-subject', 'I-subject'), ('发', 'I-subject', 'I-subject'), ('育', 'I-subject', 'I-subject'), ('正', 'I-subject', 'I-subject'), ('常', 'I-subject', 'I-subject'), ('。', 'O', 'O'), ('半', 'B-body', 'O'), ('乳', 'I-body', 'O'), ('糖', 'I-body', 'O'), ('和', 'O', 'O'), ('果', 'B-body', 'O'), ('糖', 'I-body', 'O'), ('不', 'B-subject', 'O'), ('能', 'I-subject', 'O'), ('转', 'I-subject', 'O'), ('变', 'I-subject', 'O'), ('成', 'I-subject', 'O'), ('葡', 'O', 'O'), ('萄', 'O', 'O'), ('糖', 'O', 'O'), ('并', 'O', 'O'), ('能', 'O', 'O'), ('导', 'O', 'O'), ('致', 'O', 'O'), ('酸', 'B-subject', 'O'), ('中', 'I-subject', 'O'), ('毒', 'I-subject', 'O'), ('。', 'O', 'O'), ('注', 'O', 'O'), ('射', 'O', 'O'), ('肾', 'O', 'O'), ('上', 'O', 'O'), ('腺', 'O', 'O'), ('素', 'O', 'O'), ('或', 'O', 'O'), ('胰', 'O', 'O'), ('高', 'O', 'O'), ('肾', 'O', 'O'), ('上', 'O', 'O'), ('腺', 'O', 'O'), ('素', 'O', 'O'), ('后', 'O', 'O'), ('对', 'O', 'O'), ('血', 'O', 'B-item'), ('糖', 'O', 'I-item'), ('浓', 'O', 'O'), ('度', 'O', 'O'), ('无', 'O', 'O'), ('改', 'O', 'O'), ('血', 'O', 'O'), ('糖', 'O', 'O'), ('浓', 'O', 'O'), ('度', 'O', 'O'), ('儿', 'O', 'O'), ('茶', 'O', 'O'), ('酚', 'O', 'O'), ('胺', 'O', 'O'), ('正', 'O', 'O'), ('常', 'O', 'O'), ('。', 'O', 'O')]\n",
      "0 G B-disease B-disease\n",
      "1 S I-disease I-disease\n",
      "2 D I-disease I-disease\n",
      "3 Ⅰ I-disease I-disease\n",
      "4 a I-disease I-disease\n",
      "5 型 I-disease I-disease\n",
      "6 （ I-disease I-disease\n",
      "7 V I-disease I-disease\n",
      "8 o I-disease I-disease\n",
      "9 n I-disease I-disease\n",
      "10 G I-disease I-disease\n",
      "11 i I-disease I-disease\n",
      "12 e I-disease I-disease\n",
      "13 r I-disease I-disease\n",
      "14 k I-disease I-disease\n",
      "15 e I-disease I-disease\n",
      "16 病 I-disease I-disease\n",
      "21 - I-disease I-disease\n",
      "22 6 I-disease I-disease\n",
      "23 - I-disease I-disease\n",
      "24 磷 I-disease I-disease\n",
      "25 酸 I-disease I-disease\n",
      "26 酶 I-disease I-disease\n",
      "27 缺 I-disease I-disease\n",
      "28 陷 I-disease I-disease\n",
      "35 肝 B-body B-body\n",
      "37 肾 B-body B-body\n",
      "39 小 B-body B-body\n",
      "40 肠 I-body I-body\n",
      "41 低 B-subject B-subject\n",
      "47 呼 B-subject B-subject\n",
      "48 吸 I-subject I-subject\n",
      "49 困 I-subject I-subject\n",
      "50 难 I-subject I-subject\n",
      "74 血 B-body B-body\n",
      "75 小 I-body I-body\n",
      "76 板 I-body I-body\n",
      "77 功 B-subject B-subject\n",
      "78 能 I-subject I-subject\n",
      "79 障 I-subject I-subject\n",
      "80 碍 I-subject I-subject\n",
      "94 肝 B-body B-body\n",
      "95 脏 I-body I-body\n",
      "97 肾 B-body B-body\n",
      "98 脏 I-body I-body\n",
      "99 肿 B-subject B-subject\n",
      "100 大 I-subject I-subject\n",
      "103 矮 B-subject B-subject\n",
      "104 小 I-subject I-subject\n",
      "105 智 B-subject B-subject\n",
      "106 能 I-subject I-subject\n",
      "107 发 I-subject I-subject\n",
      "108 育 I-subject I-subject\n",
      "109 正 I-subject I-subject\n",
      "110 常 I-subject I-subject\n",
      "F1-score: 48.89%\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "  subject     1.0000    0.4286    0.6000        14\n",
      "     body     0.7143    0.6250    0.6667         8\n",
      " decorate     0.0000    0.0000    0.0000         1\n",
      "  disease     0.0000    0.0000    0.0000         1\n",
      "\n",
      "micro avg     0.5238    0.4583    0.4889        24\n",
      "macro avg     0.8214    0.4583    0.5722        24\n",
      "\n",
      "******************第4个数据******************\n",
      "text:  2.糖耐量试验呈现典型的糖尿病特征。\n",
      "true label:  ['O', 'O', 'B-item', 'I-item', 'I-item', 'I-item', 'I-item', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-subject', 'I-subject', 'O']\n",
      "pred label:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'O', 'O']\n",
      "[('2', 'O', 'O'), ('.', 'O', 'O'), ('糖', 'B-item', 'O'), ('耐', 'I-item', 'O'), ('量', 'I-item', 'O'), ('试', 'I-item', 'O'), ('验', 'I-item', 'O'), ('呈', 'O', 'O'), ('现', 'O', 'O'), ('典', 'O', 'O'), ('型', 'O', 'O'), ('的', 'O', 'O'), ('糖', 'B-disease', 'B-subject'), ('尿', 'I-disease', 'I-subject'), ('病', 'I-disease', 'I-subject'), ('特', 'I-subject', 'O'), ('征', 'I-subject', 'O'), ('。', 'O', 'O')]\n",
      "F1-score: 0.00%\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "  disease     0.0000    0.0000    0.0000         1\n",
      "  subject     0.0000    0.0000    0.0000         1\n",
      "     item     0.0000    0.0000    0.0000         1\n",
      "\n",
      "micro avg     0.0000    0.0000    0.0000         3\n",
      "macro avg     0.0000    0.0000    0.0000         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "#展示分析5个案例\n",
    "test_pred_labels = [['B-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['B-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'I-body', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'I-body', 'I-body', 'O', 'B-body', 'I-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'B-subject', 'I-subject', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'B-decorate', 'I-decorate', 'I-decorate', 'B-subject', 'I-subject', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-item', 'I-item', 'I-item', 'I-item', 'I-item', 'I-item', 'I-item', 'B-subject', 'I-subject', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'B-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-disease', 'I-disease', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'B-disease', 'I-disease', 'I-disease', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'B-body', 'I-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'B-body', 'I-body', 'B-subject', 'I-subject', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-item', 'I-item', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'I-subject', 'O', 'O', 'O']]\n",
    "for i in range(5):\n",
    "    print(\"******************第%d个数据******************\"%i)\n",
    "    print(\"text: \",\"\".join(test_x[i]))\n",
    "    print(\"true label: \",test_y[i])\n",
    "    print(\"pred label: \",test_pred_labels[i])\n",
    "    result=[(a,b,c) for a,b,c in zip(test_x[i],test_y[i],test_pred_labels[i])]\n",
    "    print(result)\n",
    "    index=0\n",
    "    for a,b,c in result:\n",
    "        if b==c and b!=\"O\":\n",
    "            print(index,a,b,c)\n",
    "        index+=1\n",
    "    print(\"F1-score: {:.2%}\".format(f1_score(test_y[i], test_pred_labels[i])))\n",
    "    print(classification_report(test_y[i], test_pred_labels[i],digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "IkHcwFHKiQRl",
    "outputId": "afa54678-8806-4717-a61a-acf31a0d4601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['B-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'B-body', 'I-body', 'I-body', 'I-body', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-subject', 'I-subject', 'O', 'O', 'B-body', 'I-body', 'B-subject', 'I-subject', 'I-subject', 'I-subject', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[67, 136, 213, 166, 18, 85, 94, 100, 103, 111, 196, 80, 158, 121, 232, 123, 44, 80, 61, 96, 54, 49, 103, 24, 227, 166, 77, 261, 78, 204, 135, 64, 35, 181, 223, 33, 136, 127, 65, 202, 122, 127, 56, 333, 240, 107, 152, 44, 178, 86, 245, 48, 102, 136, 247, 46, 130, 161, 113, 124, 119, 109, 86, 117, 172, 9, 12, 49, 136, 10, 11, 43, 274, 57, 125, 178, 142, 19, 58, 119, 34, 118, 95, 105, 68, 9, 132, 131, 25, 22, 8, 72, 25, 120, 213, 47, 163, 11, 83, 19, 109, 217, 106, 25, 136, 172, 83, 60, 113, 47, 232, 133, 383, 13, 38, 148, 247, 182, 260, 83, 162, 6, 129, 180, 54, 166, 290, 461, 25, 429, 54, 62, 86, 51, 17, 209, 121, 133, 93, 66, 200, 151, 209, 341, 66, 80, 148, 87, 390, 154, 9, 63, 213, 165, 218, 162, 421, 68, 66, 144, 343, 162, 93, 178, 15, 66, 249, 211, 123, 220, 96, 34, 65, 124, 16, 10, 18, 36, 20, 17, 26, 124, 22, 23, 31, 23, 11, 235, 114, 74, 275, 210, 149, 104, 195, 98, 50, 11, 250, 210, 288, 288, 72, 192, 329, 74, 215, 149, 403, 114, 98, 94, 7, 275, 148, 329, 215, 94, 235, 250, 55, 55, 177, 74, 136, 136, 177, 150, 228, 150, 275, 228, 150, 275, 150, 19, 74, 91, 91, 419, 100, 129, 129, 419, 100, 92, 39, 145, 63, 71, 63, 156, 301, 63, 89, 40, 76, 22, 198, 5, 9, 137, 167, 311, 40, 61, 61, 63, 76, 212, 60, 311, 55, 109, 178, 243, 4, 55, 120, 4, 79, 89, 130, 109, 29, 167, 71, 131, 243, 29, 145, 212, 156, 63, 196, 301, 92, 143, 14, 143, 137, 55, 196, 63, 55, 198, 120, 60, 63, 22, 39, 44, 55, 44, 70, 87, 64, 87, 64, 39, 105, 77, 105, 116, 146, 116, 146, 77, 194, 118, 88, 120, 118, 88, 120, 77, 77, 44, 38, 44, 425, 143, 62, 70, 203, 62, 32, 55, 45, 49, 101, 101, 66, 49, 55, 26, 72, 40, 60, 49, 84, 173, 42, 79, 60, 40, 22, 143, 79, 42, 54, 59, 122, 32, 22, 59, 49, 425, 36, 54, 70, 45, 26, 39, 40, 84, 66, 79, 79, 127, 40, 39, 72, 122, 173, 37, 159, 120, 159, 36, 119, 35, 37, 119, 35, 52, 24, 10, 102, 34, 102, 86, 34, 97, 97, 77, 269, 60, 60, 64, 64, 52, 77, 86, 269, 442, 29, 92, 962, 107, 216, 92, 173, 214, 134, 153, 173, 47, 214, 142, 133, 309, 130, 133, 153, 134, 216, 133, 309, 211, 211, 223, 142, 345, 130, 415, 415, 133, 107, 223, 962, 128, 236, 46, 358, 79, 128, 50, 242, 8, 18, 58, 272, 42, 11, 23, 64, 216, 79, 64, 272, 201, 201, 242, 216, 85, 50, 226, 85, 42, 43, 358, 58, 236, 350, 227, 227, 138, 138, 334, 334, 316, 96, 85, 311, 327, 106, 132, 129, 327, 166, 121, 243, 134, 167, 77, 115, 180, 158, 233, 246, 137, 157, 171, 166, 327, 62, 62, 128, 194, 51, 128, 327, 243, 101, 157, 106, 140, 140, 158, 349, 266, 137, 132, 96, 101, 145, 167, 129, 246, 51, 148, 194, 149, 233, 171, 85, 145, 219, 155, 47, 47, 77, 349, 134, 47, 266, 180, 209, 219, 121, 33, 155, 148, 209, 115, 149, 209, 164, 259, 204, 191, 108, 144, 68, 132, 150, 272, 237, 51, 417, 200, 8, 148, 27, 246, 241, 173, 36, 26, 147, 123, 176, 145, 143, 125, 178, 210, 200, 117, 125, 241, 139, 81, 153, 109, 179, 303, 302, 176, 111, 159, 174, 6, 41, 48, 167, 14, 158, 7, 40, 346, 161, 244, 10, 7, 192, 89, 7, 175, 6, 7, 129, 176, 158, 46, 174, 61, 158, 97, 28, 151, 7, 105, 245, 88, 85, 9, 7, 211, 144, 90, 49, 80, 88, 110, 50, 100, 8, 175, 23, 10, 671, 56, 120, 248, 95, 426, 38, 484, 110, 301, 26, 124, 240, 4, 416, 44, 459, 98, 218, 64, 43, 342, 142, 415, 8, 319, 95, 499, 115, 434, 174, 488, 223, 93, 258, 75, 210, 182, 238, 220, 377, 181, 199, 132, 254, 7, 190, 353, 48, 38, 40, 54, 33, 74, 34, 47, 33, 722, 146, 159, 302, 187, 265, 16, 124, 137, 116, 114, 240, 118, 54, 84, 32, 221, 119, 97, 20, 249, 408, 59, 113, 187, 100, 98, 162, 119, 77, 256, 21, 56, 149, 47, 59, 207, 8, 293, 394, 8, 13, 143, 230, 6, 264, 251, 71, 30, 74, 166, 175, 94, 104, 322, 112, 70, 283]\n",
      "[67, 136, 213, 166, 18, 85, 94, 100, 103, 111, 196, 80, 158, 121, 232, 123, 44, 80, 61, 96, 54, 49, 103, 24, 227, 166, 77, 261, 78, 204, 135, 64, 35, 181, 223, 33, 136, 127, 65, 202, 122, 127, 56, 333, 240, 107, 152, 44, 178, 86, 245, 48, 102, 136, 247, 46, 130, 161, 113, 124, 119, 109, 86, 117, 172, 9, 12, 49, 136, 10, 11, 43, 274, 57, 125, 178, 142, 19, 58, 119, 34, 118, 95, 105, 68, 9, 132, 131, 25, 22, 8, 72, 25, 120, 213, 47, 163, 11, 83, 19, 109, 217, 106, 25, 136, 172, 83, 60, 113, 47, 232, 133, 383, 13, 38, 148, 247, 182, 260, 83, 162, 6, 129, 180, 54, 166, 290, 461, 25, 429, 54, 62, 86, 51, 17, 209, 121, 133, 93, 66, 200, 151, 209, 341, 66, 80, 148, 87, 390, 154, 9, 63, 213, 165, 218, 162, 421, 68, 66, 144, 343, 162, 93, 178, 15, 66, 249, 211, 123, 220, 96, 34, 65, 124, 16, 10, 18, 36, 20, 17, 26, 124, 22, 23, 31, 23, 11, 235, 114, 74, 275, 210, 149, 104, 195, 98, 50, 11, 250, 210, 288, 288, 72, 192, 329, 74, 215, 149, 403, 114, 98, 94, 7, 275, 148, 329, 215, 94, 235, 250, 55, 55, 177, 74, 136, 136, 177, 150, 228, 150, 275, 228, 150, 275, 150, 19, 74, 91, 91, 419, 100, 129, 129, 419, 100, 92, 39, 145, 63, 71, 63, 156, 301, 63, 89, 40, 76, 22, 198, 5, 9, 137, 167, 311, 40, 61, 61, 63, 76, 212, 60, 311, 55, 109, 178, 243, 4, 55, 120, 4, 79, 89, 130, 109, 29, 167, 71, 131, 243, 29, 145, 212, 156, 63, 196, 301, 92, 143, 14, 143, 137, 55, 196, 63, 55, 198, 120, 60, 63, 22, 39, 44, 55, 44, 70, 87, 64, 87, 64, 39, 105, 77, 105, 116, 146, 116, 146, 77, 194, 118, 88, 120, 118, 88, 120, 77, 77, 44, 38, 44, 425, 143, 62, 70, 203, 62, 32, 55, 45, 49, 101, 101, 66, 49, 55, 26, 72, 40, 60, 49, 84, 173, 42, 79, 60, 40, 22, 143, 79, 42, 54, 59, 122, 32, 22, 59, 49, 425, 36, 54, 70, 45, 26, 39, 40, 84, 66, 79, 79, 127, 40, 39, 72, 122, 173, 37, 159, 120, 159, 36, 119, 35, 37, 119, 35, 52, 24, 10, 102, 34, 102, 86, 34, 97, 97, 77, 269, 60, 60, 64, 64, 52, 77, 86, 269, 442, 29, 92, 962, 107, 216, 92, 173, 214, 134, 153, 173, 47, 214, 142, 133, 309, 130, 133, 153, 134, 216, 133, 309, 211, 211, 223, 142, 345, 130, 415, 415, 133, 107, 223, 962, 128, 236, 46, 358, 79, 128, 50, 242, 8, 18, 58, 272, 42, 11, 23, 64, 216, 79, 64, 272, 201, 201, 242, 216, 85, 50, 226, 85, 42, 43, 358, 58, 236, 350, 227, 227, 138, 138, 334, 334, 316, 96, 85, 311, 327, 106, 132, 129, 327, 166, 121, 243, 134, 167, 77, 115, 180, 158, 233, 246, 137, 157, 171, 166, 327, 62, 62, 128, 194, 51, 128, 327, 243, 101, 157, 106, 140, 140, 158, 349, 266, 137, 132, 96, 101, 145, 167, 129, 246, 51, 148, 194, 149, 233, 171, 85, 145, 219, 155, 47, 47, 77, 349, 134, 47, 266, 180, 209, 219, 121, 33, 155, 148, 209, 115, 149, 209, 164, 259, 204, 191, 108, 144, 68, 132, 150, 272, 237, 51, 417, 200, 8, 148, 27, 246, 241, 173, 36, 26, 147, 123, 176, 145, 143, 125, 178, 210, 200, 117, 125, 241, 139, 81, 153, 109, 179, 303, 302, 176, 111, 159, 174, 6, 41, 48, 167, 14, 158, 7, 40, 346, 161, 244, 10, 7, 192, 89, 7, 175, 6, 7, 129, 176, 158, 46, 174, 61, 158, 97, 28, 151, 7, 105, 245, 88, 85, 9, 7, 211, 144, 90, 49, 80, 88, 110, 50, 100, 8, 175, 23, 10, 671, 56, 120, 248, 95, 426, 38, 484, 110, 301, 26, 124, 240, 4, 416, 44, 459, 98, 218, 64, 43, 342, 142, 415, 8, 319, 95, 499, 115, 434, 174, 488, 223, 93, 258, 75, 210, 182, 238, 220, 377, 181, 199, 132, 254, 7, 190, 353, 48, 38, 40, 54, 33, 74, 34, 47, 33, 722, 146, 159, 302, 187, 265, 16, 124, 137, 116, 114, 240, 118, 54, 84, 32, 221, 119, 97, 20, 249, 408, 59, 113, 187, 100, 98, 162, 119, 77, 256, 21, 56, 149, 47, 59, 207, 8, 293, 394, 8, 13, 143, 230, 6, 264, 251, 71, 30, 74, 166, 175, 94, 104, 322, 112, 70, 283]\n"
     ]
    }
   ],
   "source": [
    "wronglist = []\n",
    "for i in range(787):\n",
    "  if len(pred_final[i]) != len(test_y[i]):\n",
    "    wronglist.append([len(pred_final[i]), len(test_y[i])])\n",
    "print(wronglist)\n",
    "print(len(wronglist))\n",
    "print(pred_final[0])\n",
    "print(test_y[0])\n",
    "ylen = [len(_)for _ in test_y]\n",
    "predlen = [len(_) for _ in pred_final]\n",
    "print(ylen)\n",
    "print(predlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "GV17cttOZYv2",
    "outputId": "faca5533-b91c-41b8-af91-71e00c0c1221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.10)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.16.4)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "8lwf5BR8ZcxN",
    "outputId": "4667133c-1894-4a07-e7bd-c899042a8894"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 40.76%\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "  subject       0.42      0.41      0.42      3011\n",
      "frequency       0.50      0.12      0.19        50\n",
      "  disease       0.32      0.40      0.36       610\n",
      "     item       0.24      0.18      0.20       177\n",
      "     body       0.41      0.50      0.45      1448\n",
      " decorate       0.47      0.29      0.36       632\n",
      "\n",
      "micro avg       0.41      0.41      0.41      5928\n",
      "macro avg       0.41      0.41      0.40      5928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "print(\"F1-score: {:.2%}\".format(f1_score(test_y, pred_final)))\n",
    "print(classification_report(test_y, pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "aGumBRMuZmCp",
    "outputId": "403cdb39-f1f8-4fab-c74e-f6f4ff5f246e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn_crfsuite\n",
      "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (4.41.1)\n",
      "Collecting python-crfsuite>=0.8.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
      "\u001b[K     |████████████████████████████████| 747kB 6.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.8.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (1.12.0)\n",
      "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n"
     ]
    }
   ],
   "source": [
    "! pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "LVmkv7m3ZoGz",
    "outputId": "5abf17cf-49c5-415d-9878-ac557f55c189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-body       0.45      0.55      0.49      1445\n",
      "  B-decorate       0.52      0.32      0.40       632\n",
      "   B-disease       0.36      0.43      0.39       610\n",
      " B-frequency       0.50      0.12      0.19        50\n",
      "      B-item       0.34      0.20      0.25       177\n",
      "   B-subject       0.47      0.46      0.47      2931\n",
      "      I-body       0.44      0.56      0.49      2435\n",
      "  I-decorate       0.53      0.30      0.38      1260\n",
      "   I-disease       0.38      0.49      0.42      2496\n",
      " I-frequency       0.50      0.06      0.11       101\n",
      "      I-item       0.31      0.16      0.21       380\n",
      "   I-subject       0.53      0.37      0.44      7269\n",
      "           O       0.89      0.91      0.90     87901\n",
      "\n",
      "    accuracy                           0.82    107687\n",
      "   macro avg       0.48      0.38      0.40    107687\n",
      "weighted avg       0.82      0.82      0.82    107687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from  sklearn_crfsuite.metrics import flat_classification_report  \n",
    "report = flat_classification_report(y_pred=pred_final, y_true=test_y)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bert_BiLSTM_Crf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
